description: Naia OS hybrid architecture (mirror of .users/context/architecture.md)

design_philosophy: |
  Naia OS is NOT built from scratch. It's a hybrid assembly of 3 proven ecosystems:
  OpenClaw (Gateway/exec), project-careti (LLM/tools), OpenCode (architecture pattern).
  Each contributes its strongest capability. The result is greater than any single source.

why_hybrid:
  pure_openclaw_problems:
    - "CLI-only UX — no avatar, no visual feedback, no emotion"
    - "No Tauri/desktop-native shell — terminal-first design"
    - "Tight coupling to Claude Code as the only agent model"
    - "No multi-provider LLM support (Gemini, xAI, etc.)"
  pure_careti_problems:
    - "VS Code extension — not an OS, not always-on"
    - "No daemon architecture — dies when IDE closes"
    - "No external channels (Discord, Telegram, etc.)"
    - "No skills system, no memory persistence"
  pure_opencode_problems:
    - "TUI-only — no VRM avatar, no desktop app"
    - "No Gateway/daemon concept"
    - "No channel integration"
  hybrid_solution: |
    Take OpenClaw's daemon+exec+channels+skills ecosystem as the runtime backend.
    Take Careti's multi-provider LLM+tool definitions+stdio protocol as the agent core.
    Take OpenCode's client/server architecture pattern for clean separation.
    Wrap it in a Tauri desktop shell with VRM avatar for accessible UX.

three_pillars:
  pillar_1_openclaw:
    role: "Runtime backend (daemon + execution environment)"
    provides:
      - "Gateway WebSocket server (systemd, always-on)"
      - "Command/file RPCs (exec.bash, node.invoke/system.run)"
      - "Security: device auth, token scopes, exec approvals"
      - "Channels: Discord, Telegram, WhatsApp, Slack, IRC, etc."
      - "Skills: 50+ built-in (weather, time, memo, etc.)"
      - "Memory: conversation persistence, context recall"
      - "Sessions: multi-session management, sub-agent spawn"
      - "ACP (Agent Control Protocol): client↔agent bridge"
      - "TTS: unified provider selector (Edge TTS free, Google Cloud, OpenAI, ElevenLabs) with direct API calls"
    connection_protocol:
      handshake: "connect.challenge → connect (protocol v3)"
      auth: "device identity + token scopes (Ed25519 signed nonce)"
      client_id: "cli (paired device identifier)"
      methods: "88 methods (agent, sessions.*, exec.*, skills.*, channels.*, etc.)"
    what_naia_uses:
      - "Gateway daemon as systemd user service"
      - "Command execution via exec.bash (preferred) or node.invoke fallback"
      - "exec.approvals for permission system"
      - "sessions.* for session management (when exposed by gateway config)"
      - "skills.invoke and/or browser.request for web/browser actions"
      - "channels.* for Discord/Telegram"
      - "agent method for agent interaction"

  pillar_2_careti:
    role: "Agent intelligence (LLM + tools + persona)"
    provides:
      - "Multi-provider LLM: Gemini (default), xAI (Grok), Claude"
      - "Tool definitions: GATEWAY_TOOLS (8 tools)"
      - "Function calling: Gemini native (xAI/Claude = tech debt)"
      - "Alpha persona: system prompt, emotion mapping"
      - "Cost tracking: per-request cost display"
      - "stdio JSON lines protocol: Shell ↔ Agent"
    what_naia_uses:
      - "LLM providers (gemini.ts, xai.ts, anthropic.ts)"
      - "Tool loop (max 10 iterations, abort support)"
      - "Permission tiers 0-3"
      - "stdio protocol (chat_request/text/tool_use/tool_result/etc.)"

  pillar_3_opencode:
    role: "Architecture pattern (client/server separation)"
    provides:
      - "Clean client/server separation"
      - "Provider abstraction layer"
      - "LSP-style communication pattern"
    what_naia_uses:
      - "Shell (client) / Agent (server) separation via stdio"
      - "Provider factory pattern (buildProvider)"
      - "Clean module boundaries (shell/agent/gateway)"

runtime_architecture:
  diagram: |
    ┌─────────────────────────────────────────────────────────┐
    │  Naia Shell (Tauri 2 + React + Three.js VRM Avatar) │
    │  Role: Desktop UI, avatar rendering, chat panel        │
    │  Source: Naia + AIRI (VRM) + shadcn/ui              │
    └──────────────────────┬──────────────────────────────────┘
                           │ stdio JSON lines
    ┌──────────────────────▼──────────────────────────────────┐
    │  Naia Agent (Node.js)                                │
    │  Role: LLM connection, tool orchestration, Alpha persona│
    │  Source: Careti providers + OpenCode pattern             │
    │  Features: multi-LLM, TTS, emotion, cost tracking       │
    └──────────────────────┬──────────────────────────────────┘
                           │ WebSocket (ws://127.0.0.1:18789)
    ┌──────────────────────▼──────────────────────────────────┐
    │  OpenClaw Gateway (systemd user service)                │
    │  Role: Execution, security, channels, skills, memory    │
    │  Source: OpenClaw ecosystem (npm: openclaw)              │
    │  Auth: device identity + token scopes (protocol v3)     │
    │  Methods: dynamic by profile (agent, node.invoke,        │
    │  sessions.*, browser.request, skills.*, channels.* ...)  │
    └─────────────────────────────────────────────────────────┘

  data_flow:
    chat: "User → Shell → Agent → LLM → Agent → Shell → User"
    tool_exec: "LLM → Agent (tool_use) → Gateway (exec.bash or node.invoke) → OS → result → LLM"
    approval: "Gateway → Agent (approval_request) → Shell (modal) → user decision → Agent → Gateway"
    external: "Discord msg → Gateway → Agent → LLM → Agent → Gateway → Discord reply"

desktop_avatar_asset_pipeline:
  purpose: "Stable local-file loading for VRM model/background in Tauri WebView"
  rules:
    - "Normalize file:// paths to absolute filesystem paths before storage/rendering."
    - "If URL appears as http://localhost/... convert to http://asset.localhost/... for Tauri asset protocol compatibility."
    - "For absolute local VRM, read bytes via Rust command read_local_binary and parse as ArrayBuffer in frontend to avoid CORS/fetch failures."
    - "Background images may use asset URL conversion; fallback to gradient if loading fails."
  e2e_note:
    - "e2e-tauri uses prebuilt binary src-tauri/target/debug/naia-shell (not pnpm build output)."
    - "When Rust command/invoke_handler changes, run cargo build in src-tauri before wdio e2e."

channel_onboarding_routing:
  description: "Discord bot add flow is routed through naia.nextain.io instead of direct webhook input"
  routes:
    - "Shell ChannelsTab login(discord) -> https://naia.nextain.io/ko/discord/connect?source=naia-shell"
    - "Onboarding complete optional action -> same route"
  security:
    - "Do not expose/use DISCORD_BOT_TOKEN in shell frontend"
    - "Bot secrets stay server-side in naia.nextain.io env"

deep_link_persistence_contract:
  description: "OAuth deep-link payload must be persisted even when specific UI tabs are not mounted"
  requirements:
    - "Deep-link events that affect runtime behavior (e.g., discord_auth_complete) must be handled by an always-mounted layer (App root or equivalent)."
    - "Settings/Onboarding listeners may update local UI state, but persistence logic must be centralized in shared lib code."
    - "Agent send default-target behavior must not depend on whether Settings tab was open during OAuth callback."
  anti_patterns:
    - "Persisting critical auth payload only inside feature tabs/components that can be unmounted."
    - "Duplicating persistence rules across multiple components with divergent fallback behavior."

memory_architecture:
  description: "2-tier memory system for Alpha personal AI agent"
  tiers:
    short_term:
      name: "Short-Term Memory (STM)"
      storage: "Zustand (in-memory) + SQLite messages table"
      scope: "Current session full messages"
      lifetime: "Current session ~ recent 7 days"
      implementation:
        rust: "memory.rs (sessions + messages tables, Arc<Mutex<Connection>>)"
        frontend: "db.ts (invoke wrappers) + chat store (sessionId, loadSession)"
    long_term:
      name: "Long-Term Memory (LTM)"
      storage: "SQLite (sessions.summary + facts table)"
      scope: "Cross-session knowledge"
      sub_types:
        episodic: "Session summaries (LLM-generated, stored in sessions.summary)"
        semantic: "User facts/preferences (extracted by LLM, stored in facts table)"
  search_evolution:
    phase_4_4a: "SQLite LIKE (keyword matching)"
    phase_4_4b: "SQLite FTS5 BM25 (full-text search)"
    phase_4_5: "Gemini Embedding API (semantic search)"
    phase_5_plus: "sLLM local embedding (Ollama, llama.cpp)"
  interface: |
    MemoryProcessor {
      summarize(messages): Promise<string>
      extractFacts?(messages): Promise<Fact[]>
      semanticSearch?(query, limit): Promise<MessageRow[]>
    }
  db_schema:
    sessions: "id TEXT PK, created_at INT, title TEXT, summary TEXT"
    messages: "id TEXT PK, session_id TEXT FK, role TEXT, content TEXT, timestamp INT, cost_json TEXT, tool_calls_json TEXT"
    facts_planned: "id TEXT PK, key TEXT, value TEXT, source TEXT, updated_at INT"

security_layers:
  layer_1_os: "Bazzite immutable rootfs + SELinux"
  layer_2_gateway: "OpenClaw device auth + token scopes + exec approvals"
  layer_3_agent: "Permission tiers 0-3 + tool-level blocking"
  layer_4_shell: "User approval modal + tools on/off toggle"
  principle: "Defense in depth. Every layer is independent."

gateway_client_protocol:
  description: "How Naia Agent connects to OpenClaw Gateway"
  steps:
    1_ws_connect: "WebSocket connect to ws://127.0.0.1:18789"
    2_challenge: "Receive connect.challenge event with nonce"
    3_handshake: "Send connect request with auth token, protocol v3, client info"
    4_hello_ok: "Receive hello-ok with available methods and features"
    5_rpc: "Send/receive req/res frames for tool execution"
  auth_params:
    token: "gateway.auth.token from config (shared secret)"
    client_id: "cli (matches paired device clientId)"
    platform: "linux"
    mode: "cli"
    min_protocol: 3
    max_protocol: 3
